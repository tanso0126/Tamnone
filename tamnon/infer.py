from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

MODEL_PATH = "./output"  # train.pyì—ì„œ ì €ì¥í•œ ê²½ë¡œ

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map="auto")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

def ask(instruction, input_text):
    prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n"
    result = pipe(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)[0]["generated_text"]
    print(result.split("### Response:\n")[-1].strip())

# ğŸ” í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
ask("ê°ˆë“± ë°œí™”ë¥¼ ë¶„ì„í•˜ê³ , ê°ˆë“±ì´ í•´ê²°ë  ìˆ˜ ìˆë„ë¡ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.",
    "í‰ì†Œì— ê±°ì§€ì²˜ëŸ¼ ì…ê³ ë‹¤ë‹ˆëŠ” ì§ì¥ ë™ë£Œê°€ ê¹”ë”í•œ ìƒˆ ì˜·ì„ ì…ê³  ì¶œê·¼í•˜ëŠ”ë° ì–´ë””ì„œ í›”ì¹œ ê²ƒ ê°™ì§€ ì•Šë‚˜ìš”?")
